{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ab0626",
   "metadata": {},
   "source": [
    "# Redes Neuronales Convolucionales con imagenes de perros y gatos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467a379",
   "metadata": {},
   "source": [
    "## Parte 1.- Construir el modelo de CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059684d0",
   "metadata": {},
   "source": [
    "### Importar las librerías y paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "070fda4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D,MaxPooling2D,Conv2D #imagenes en 2D\n",
    "from keras.layers import Flatten,Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8cb077",
   "metadata": {},
   "source": [
    "### Inicializar la CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c728adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cbd10",
   "metadata": {},
   "source": [
    "### Paso 1.- Capa de convolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c791790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel es el tamaño de la matriz, filters los fltros que van a ser , n_filters son los detectores de rasgos\n",
    "#Para nuestro caso tenemos 32 ventanas de caracteristicas en matriz de 3x3\n",
    "#Ponemos las imagenes en tamaño pequeño para menor costo computacional\n",
    "#Debemos pensar si es necesario tener nuestros canales de color\n",
    "#Imagenes de 64x64 y 3 canales de color \n",
    "#Usamos el rectificador lineal uniforme\n",
    "classifier.add( Conv2D( filters=32,kernel_size=(3,3),input_shape = (64, 64, 3),activation= \"relu\") )#Convolución en 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070b460",
   "metadata": {},
   "source": [
    "### Paso 2.- Capa de Max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff593dea",
   "metadata": {},
   "source": [
    "El maximo del valor de cada ventana con respecto a una matriz, haciendo que los mapas de caracteristicas sean mas chicos y además toma en cuenta si la imagen está rotada o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba73746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#una matriz de 2x2\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2) )) #capa de MAX Pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6992953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segunda capa de convolucion y max pooling, ya no especificamos el tamaño de entrada\n",
    "classifier.add( Conv2D( filters=32,kernel_size=(3,3),activation= \"relu\") )\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa6682",
   "metadata": {},
   "source": [
    "### Paso 3.- FLattening (aplanado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9721d3",
   "metadata": {},
   "source": [
    "Pasar las matrices a un vector de 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13315f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten()) #Capa de aplanado, tenemos un vector unico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b7833",
   "metadata": {},
   "source": [
    "### paso 4.- full connection, capa totalmente conectada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc3db227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#units de la capa de oculta, 128 nodos en la primer capa oculta, es decir varias neuronitas (circulos)\n",
    "#una neurona se despierta con la cantidad de señal\n",
    "classifier.add(Dense(units =128,activation=\"relu\")) #Capa de aplancapa oculta\n",
    "#Probabilidad de ser perro o ser gato, por eso solo 1 neuronita\n",
    "classifier.add(Dense(units =1,activation=\"sigmoid\")) #Capa de salida 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b3027",
   "metadata": {},
   "source": [
    "### Paso 5.- Compilar la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b8310ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizador de Adam, mejor que gradiente descendiente\n",
    "#funcion de perdidad (loss), eligimos una binaria, pero si fueran mas, sería categorica\n",
    "#Usamos accuracy como metrica\n",
    "classifier.compile(optimizer =\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f69c4b",
   "metadata": {},
   "source": [
    "## Parte 2.- Ajustar la red Neuronal Convolucional a las imagenes para entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc06a8",
   "metadata": {},
   "source": [
    "Podemos hacer transformaciones de imagenes, reescalandolas, volteandolas, haciendo zoom, etc. Todo eso hace que tebngamos mas datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abd2d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cfea42",
   "metadata": {},
   "source": [
    "#### Cargamos las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1954aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#20% mas zoom\n",
    "train_datagen = ImageDataGenerator(rescale= 1./255,shear_range=0.2,horizontal_flip=True) \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#batch_size, se cargan de 32 en 32 en la red neuronal\n",
    "training_dataset = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                        target_size = (64, 64),batch_size =32,class_mode='binary')\n",
    "testing_dataset = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                                        target_size = (64, 64),batch_size =32,class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6a66570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-39bd43bf8d5b>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  classifier.fit_generator(training_dataset, #conjunto de entrenamiento\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6609 - accuracy: 0.6020WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n",
      "250/250 [==============================] - 127s 501ms/step - loss: 0.6609 - accuracy: 0.6020 - val_loss: 0.5807 - val_accuracy: 0.7005\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.5782 - accuracy: 0.6961\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.5202 - accuracy: 0.7411\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.4819 - accuracy: 0.7624\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.4569 - accuracy: 0.7832\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 0.4169 - accuracy: 0.8084\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 33s 130ms/step - loss: 0.3989 - accuracy: 0.8207\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 0.3592 - accuracy: 0.8426\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 0.3363 - accuracy: 0.8537\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.3032 - accuracy: 0.8675\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.2686 - accuracy: 0.8890\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.2339 - accuracy: 0.9039\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.1983 - accuracy: 0.9208\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.1644 - accuracy: 0.9389\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.1441 - accuracy: 0.9467\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.1177 - accuracy: 0.9566\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.0909 - accuracy: 0.9699\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.0773 - accuracy: 0.9725\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.0648 - accuracy: 0.9791\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.0531 - accuracy: 0.9816\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.0475 - accuracy: 0.9843\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 0.0337 - accuracy: 0.9893\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.0358 - accuracy: 0.9885\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.0243 - accuracy: 0.9944\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.0224 - accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1929bc1e9a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_dataset, #conjunto de entrenamiento\n",
    "            steps_per_epoch = training_dataset.n//32, #Muestras que toma en cada ciclo de entrenamiento, pasaremos todas las imagenes\n",
    "            epochs=25, #Cuantas epocas usaremos para entrenar\n",
    "            validation_data=testing_dataset, #Conjunto de validación\n",
    "            validation_steps=2000) #Cada cuantos pasadas validaremos nuestro resultado en este caso 2 cada 8 epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a11d7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('perros_gatos.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88be3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model('perros_gatos.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb5ac3d",
   "metadata": {},
   "source": [
    "### OBtener resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f096fe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-1b747758e020>:9: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  predictions = classifier.predict_generator(testing_dataset, steps=test_steps_per_Epoch)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"test_generator = ImageDataGenerator(rescale=1./255)\n",
    "test_data_generator = test_generator.flow_from_directory(\n",
    "   'dataset/test_set', \n",
    "     target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\"\"\"\n",
    "test_steps_per_Epoch = np.math.ceil(testing_dataset.samples / testing_dataset.batch_size) #retorna la lista en enteros\n",
    "\n",
    "predictions = classifier.predict_generator(testing_dataset, steps=test_steps_per_Epoch)\n",
    "# Get most likely class\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bce37ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = testing_dataset.classes\n",
    "class_labels = list(testing_dataset.class_indices.keys())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c88b796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'dogs']\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(class_labels)\n",
    "print(true_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3f7bde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cats       0.50      1.00      0.67      1000\n",
      "        dogs       0.00      0.00      0.00      1000\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.25      0.50      0.33      2000\n",
      "weighted avg       0.25      0.50      0.33      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfc993e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Para cargar solo una imagen\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca2b7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos la imagen, y se convierte a 64x64\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg',target_size = (64, 64))\n",
    "#modificamos la imagen en 2 dimensiones que pase, lo pasamos a array, debemos indiciar también la dimension de cuantas imagenes\n",
    "hay\n",
    "test_image = image.img_to_array\n",
    "#expandimos la dimension, y se convierte en 4 dimensiones\n",
    "test_image = np.expand_dims(test_image,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13b6f114",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'function'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-b8c1de0cd31f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    986\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m     \u001b[1;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m    989\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m         \"input: {}, {}\".format(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'function'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "xd = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275219a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
